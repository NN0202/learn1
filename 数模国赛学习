##基于原理如何创新
------评价与分析类-----------------------------------------------
#层次分析法
  题型：无数据，主观确定权重
  原理要点：构建判断矩阵
  实现步骤：0）确定目标，准则，方案层 1）用0-9标度构建判断矩阵，奇数标度，偶数折中 2）数据预处理：行归一化 3）一致性检验：CI=λmax-n/n-1，RI随矩阵阶数变化，
           CR=CI/RI <0.1则合格 4）计算权重：算术/几何平均/特征值法

#模糊综合评价法
  题型：无数据，已知权重排序方案
  原理要点：构建模糊矩阵
  实现步骤：0）确定评分，准则集 1）1.对于一个方案，模糊矩阵行为指标，列为评语；2.对于多个方案，模糊矩阵行为方案，列为指标，每个数值代表一个评语 2）确定指标权重（其他方法） 
           3）计算模糊向量R=模糊矩阵⭕权重，对于一/多个方案行列同上 4）计算综合得分：S=R*级分
           对于有二级指标的：3）计算每个一级指标的Ri：Ri=模糊矩阵i⭕二级权重 4）计算Rj：Rj=模糊矩阵j（每行由Ri构成）⭕一级权重
#主成分分析法
  题型：数据特征降维（向量投影）（新维度带有主观性、模糊性）-客观求权重
  原理要点：用方差计算并衡量变量之间相关性，一个主成分即为数据集中的一个方向，数据方差最大的方向（all原始变量的线性组合，用向量几何意义理解），主成分之间无相关性
  实现步骤：0）数据预处理 1）构建协方差矩阵（对角线上是方差） 2）构建其特征矩阵（按列来），特征向量即主成分方向，撑起新的线性空间。每行是每个原始变量，一行的值代表这个变量在每个主成分的权重（载荷）
特征值表示每个主成分的方差 3）选择主成分数量（累计贡献率小于80%所对应的前几个量）并计算综合得分（信息贡献率=λ/Σλ-累计信息贡献率-综合得分） 4）写出主成分并分析主成分是什么变量（看载荷绝对值）
#因子分析法
  题型：分类数据变量，一定程度上实现数据特征降维
  原理要点：通过提取因子来解释变量之间的相关性
  实现步骤：0）数据预处理 1）确定公共因子（主观），求目标变量的总体均值，特殊因子（残差），建立模型X目标变量=μ+ΣαF+ε 2）求因子载荷（最大似然估计法）：原始载荷矩阵=根号下特征值*特征向量，因子载荷=原始载荷矩阵*旋转因子矩阵（为了扩大载荷之间差异，得到合理解释） 
          3）计算因子得分 4）将因子得分视为自变量，继续后续处理：聚类、回归分析等
#秩和比综合评价法
  题型：已知权重排序方案
  原理要点：根据正负向指标中数据的排位，知道一个方案在所有方案中相对位置，从而排序方案
  实现步骤：1）整数次或非整数次编秩，相同值按算术平均进行编秩 2）计算每个方案的加权RSR，RSR=Σw*Rij/m*n，Rij表示第 i 行 第 j 列元素的秩。即对于每个评价对象，我们求和其m个评价指标上的秩次Rij 3）编制RSR频率分布表（每个方案的频数，累计频数，累计频率p
           将p转化为probit概率单位），累积频数表示在评价对象中，RSR值小于或等于当前评价对象RSR值的总个数 4）计算直线回归方程中系数ab RSR=a+b*Probit 5）分级：RSR值在40%以下的被归为C等，40%到80%B 超过80% A
#TOPSIS
  题型：有数据，已知权重排序方案
  原理要点：计算每个方案到正负理想解的欧式几何距离，
  实现步骤：0）数据预处理 1）代入权重，求加权矩阵 2）求正负理想解，和每个方案到正、负理想解的欧式几何距离3）计算贴合度C=(d-)/(d-)+(d+)
#灰色关联分析
  题型：已知指标权重，排序指标关联度（不是综合评价体系，指标即方案）
  原理要点：计算每个指标和目标指标L1范数距离
  实现步骤 0）数据预处理：一般是初值化归一化、均值归一化 1）计算灰色关联系数：x=[min（i）min（k）|x(0（目标）)-x(i)|+ρmax(i)max(k)|x(0)-x(i)]/[|x(0)-x(i)|+ρmax(i)max(k)|x(0)-x(i)]
           分子第一项表示与目标的距离，越小关联度越大；第二项调节输出的分布稀疏或紧密ρ越小，区分度越大；分式形态以归一化[0,1] 2）计算灰色加权关联度：r=Σw*x
  代码：%读取数据
a=xlsread('data.xlsx');
 
%无量纲化
for i =[1:7] 
a(i,:)=(a(i,:)-min(a(i,:)))/(max(a(i,:))-min(a(i,:))) 
end
 
%选出参考序列
t = max(a.');
t = repmat(t.',1,5); 
 
%灰色相关系数 
m = t-a; 
mmin = min(min(m)); 
mmax = max(max(m));
rho = 0.5; 
coefficient = (mmin + rho*mmax)./(m + rho*mmax); 
 
%权重
w = [0.08265, 0.02755, 0.13156, 0.52624, 0.02429, 0.14778, 0.05993];  
 
%灰色关联度
correlation = w * coefficient; 
#熵权法
  题型：有数据，客观求权重
  原理要点：计算每个指标的信息熵
  实现步骤：0）数据预处理 1）信息熵：pij比重=aij/Σaij（同列所有行） ej=-1/ln(n)*Σpij*ln(pij) dj=1-ej w=dj/Σdj（同列所有行）
           一级指标=同一一级指标的二级指标权重的加权平均，再归一化
#数据包络分析
  题型：指标有决策单元、输入和输出（函数），评价决策单元是否有效（顺带排序），（如生产效率问题）
  原理要点：确定每个DMU在效率前沿面上的位置，效率前沿由给定条件输出max or输入min组成
  实现步骤 0）数据预处理 1）线性规划模型：maxh=Σuy（投入一定），Σuy-Σvx<0，Σvx=1, u,v>0 求得最优的权重和决策单元效率。若h=1，便是具有效率的 2）转化为对偶模型：minθ， Σλxi <= θx0 ，Σλyi >= y0
           Θ是缩放比例，=1有效，>1无效

  
#专门确定权重的方法：聚类分析法，宁多勿漏
-----预测类----------------------------------------
系统预测原理：一、惯性原理：数据本质特则随时间变化会保留；二、类推原理：1）相似性原则 2）因果关系原则

#BP神经网络（建立输入-输出映射关系）
前向传播（pn=输入的线性组合+bn，⭐On激活函数=f(pn)非线性化，y'=on的线性组合）-计算损失（误差）J=(y‘-y)**2-反向传播（梯度下降算法更新参数）θ参数new=θold-n*▼J(θ)

#灰色预测

#时间序列
  题型：短期预测，预测变量随时间变化
  指数平滑法原理要点：1.预测的数据是由过去数据加权平均得到（贡献度不同）x_(t+1)=Σa_0 x_(t)....,Σa =1
                    2.权重系数是指数变化的a=α（1-α）i次方，i=1~t-1
                    3.权重系数公式中的超参数α的选取：0.8注重近期数据，0.3注重历史数据 0.5默认
                   一次指数平滑公式：St=αy+α（1-α）S(t-1) 其中y是t时刻数据实际值（观测值），s（t-1）隐含历史数据实际值的加权平均。St叫t时刻数据平滑值（估计值）
                   二次指数平滑公式：Lt=αy+α（1-α）（L(t-1) +T(t-1))  Tt=β(Lt-L(t-1))+（1-β)T(t-1），最终预测值y=Lt+hTt，h是预测步长
 
   ARMA原理要点：1.AR：过去数据观测值的加权平均+白噪声项
                2.MA：过去数据误差值的加权平均+白噪声项
                3.预测值y=c+AR+MA+当前误差项
   ARIMA原理要点：1.差分后得到平稳序列
                 2.分析平稳序列的ACF和PACF，进而估计模型参数
                 3.用上述参数，建立ARMA模型
                 4.评价指标用MSE MAE R²
   实现步骤：数据预处理：折线图确定不平稳→ADF检验：若不平稳则要先进行d阶差分运算，化为平稳时间序列；若平稳则进行白噪声检验，非白噪声序列，则继续→ 确定模型参数：ACF+PACF或AIC BIC或PSO→模型预测和检验
   代码框架：
from __future__ import annotations
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller as ADF
from statsmodels.stats.diagnostic import acorr_ljungbox
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf #ACF与PACF
from statsmodels.tsa.arima.model import ARIMA #ARIMA模型
from statsmodels.graphics.api import qqplot  #qq图
from scipy import stats
data=pd.read_excel('path')
#ADF检验
draw_acf_pacf(data,lags=50)
def teststationarity(data, max_lag=None):
    dftest = statsmodels.tsa.stattools.adfuller(ts, maxlag=max_lag)
    # 对上述函数求得的值进行语义描述
    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])
    for key, value in dftest[4].items():
        dfoutput['Critical Value (%s)' % key] = value
    if(dfoutput['Test Statistic'] < dfoutput['Critical Value (1%)']):
        print("ADF平稳性检验:")
        print("****** Test Statistic < Critical Value (1%) ******\n             此序列为平稳序列")
    return dfoutput    
print(teststationarity(ts))# 平稳性检验
# 进行差分处理
diff1 = data.diff().dropna()#一阶差分
diff2 = diff1.diff().dropna()#二阶差分
diff3 = diff2.diff().dropna()#三阶差分
#白噪声检验
def test_stochastic(data, lags=24, alpha=0.05):#返回统计量和p值  lags为检验的延迟数
    p_value = acorr_ljungbox(data, lags=lags) #lags可自定义
    print(p_value)
    if np.max(p_value['lb_pvalue']) < alpha:
        return '该序列不是白噪声序列'
    else:
        return '该序列是白噪声序列'
    return p_value
    #返回统计量和p值,lb_pvalue>0.05则为白噪声序列
print(test_stochastic(data,lags=3))
# # 提取序列的趋势、季节和随机效应（残差）
#分解成趋势（trend）季节性（seasonality）和残差（residual）****************************
import statsmodels.api as sm
res = sm.tsa.seasonal_decompose(data,period=7,model="add")
fig = res.plot()
# 调整图的大小
fig = plt.gcf()
fig.set_size_inches(15, 8)
# # 自相关ACF，偏自相关PACF图像
def draw_acf_pacf(ts,lags=12):
    f = plt.figure()
    ax1 = f.add_subplot(211)
    plot_acf(ts,ax=ax1,lags=lags,color = 'royalblue')
    plt.title("自相关性",fontdict={'weight':'normal','size': 15})
    ax2 = f.add_subplot(212)
    plot_pacf(ts,ax=ax2,lags=lags,color = 'royalblue')
    plt.title("偏自相关性", fontdict={'weight': 'normal', 'size': 15})
    # 调整图的大小
    fig = plt.gcf()
    fig.set_size_inches(15, 8)
    #plt.subplots_adjust(hspace=0.5)
    plt.show()


         

#回归分析（适合长期）
#预测类模型检验方法：
回归：MAPE RMSE
分类：A准确率 R召回率 P精确率


通用代码：import math  
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
import matplotlib.pyplot as plt
import pandas as pd
data=pd.read_excel('文件名',usecols=['','',''],skiprows=跳过前几行,nrows=读取前几行（除跳过外）,index_cols='')   #header默认第一行为列名，header=None则无列名  names=新列名，index_col默认自动生成一列索引
data.rename(columns={'data':'deal_data', 'time':'time_data'}, inplace = True)#更改列名
data.set_index(['time_data'], inplace=True)#设置索引
train=data[:int(len(data)*0.8)]
test=data[int(len(data)*0.8):]

plt.plot(x,y)#折线图 plt.bar(x,y)#条形图  plt.boxplot(data)#箱型图 plt.hist(data,bins=几个区间)#直方图
plt.xlabel('') plt.ylabel('') plt.title('') 
plt.show()
         

